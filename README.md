# Outline
This preprocessing pipeline is an all-relevant feature selection method that integrates two existing methods: LASSOCV (from the sklearn Python package) and BorutaPy (from the boruta_py Python package). This integration emphasizes how the two existing methods are complementary. LASSOCV captures the linearly relevant feature subset through L1-regularization. However, it can commit a type-II error when a feature is not relevant in a linear manner. Analogously, Boruta, embeds an ensemble-based method random forest model to identify relevant features. Since it omits linearity, it can analogously commit a type-II error when a feature is linearly relevant but not deemed relevant by the majority votes from its decision trees. So, this integration aims to capture both linearly and non-linearly relevant features simultaneously.

In the original work by Kursa and Rudnicki (2010), shadow features are created by shuffling the samples of the (real) features in Boruta. While their condition requires that a real feature is deemed important only if it is significantly more important than the best shadow feature (i.e., the shadow feature with the highest importance score), the boruta_py implementation relaxes this condition by allowing the user to tune the hyperparameter _perc_ such that each real feature is compared to a specific percentile of the shadow feature set. In future work, we will look at how hyperparameter optimization can be performed for _perc_. This is potentially useful because users may want to compare how different feature subsets yield different modeling results (even at the cost of Type-I error when a larger feature subset is considered). 

Additionally, note that Boruta is a _greedy_ method in the sense that once a feature is deemed relevant/irrelevant in a given iteration, later iterations will not revoke this judgment. Though this helps reduce the computational complexity (by decreasing the number of trees in the random forest in later iterations), this greedy approach leaves a large room for error in hypothesis testing (particularly when $p >> n$). That is, an irrelevant feature can be _luckily_ deemed relevant in some small number of initial iterations. The converse is also true for relevant features. A less greedy approach might help lower the probabilities of type-I/type-II errors by relying less on hypothesis testing from initial iterations. This will also be examined in the future.  

# How to use the pipeline
Users are suggested to use the original codes (from sklearn or boruta_py) directly if they are interested in only the result of LASSOCV or Boruta but not both. Also, the script doubleImpact.py assumes that the user has already installed the boruta_py Python package with the latest release on their GitHub page (https://github.com/scikit-learn-contrib/boruta_py). In particular, their boruta_py.py file can be different from the version installed by conda/pip where the latter includes deprecated Numpy commands.

Simply download/copy the script to your work directory and load the training feature set and target as X and Y respectively. After specifying a desired _scipy_ random forest estimator (i.e., classifier/regressor) as the estimator_B argument, the pipeline will return a filtered dataset with only the selected features and a summary of which feature was picked by which feature selection method. Other runtime parameters all have their default values specified as those in their original implementation in sklearn or boruta_py. The bottom of the script provided a simple example. 




