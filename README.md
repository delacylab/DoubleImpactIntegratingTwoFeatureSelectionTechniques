# Outline
This preprocessing pipeline is an all-relevant feature selection method that integrates two existing methods: [LASSOCV](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LassoCV.html) and [BorutaPy](https://github.com/scikit-learn-contrib/boruta_py). This integration emphasizes how the two existing methods are complementary. LASSOCV captures the linearly relevant feature subset through L1-regularization. It can commit a type-II error when a feature is not relevant in a linear manner. On the other hand, Boruta is an ensemble-based method proposed by Kursa and Rudnicki (2010). It used random forest models to identify relevant features. Notice that this ensemble-based method omits linearity so a type-II error can be committed analogously when a feature is only linearly relevant but deemed irrelevant by the majority votes from its decision trees. Hence, this integration aims to capture linearly and non-linearly relevant features simultaneously.

In the original work by Kursa and Rudnicki (2010), shadow features are created by shuffling the samples of the (real) features in Boruta. While their condition requires that a real feature is deemed important only if it is significantly more important than the best shadow feature (i.e., the shadow feature with the highest importance score), the boruta_py implementation relaxes this condition by allowing the user to tune the hyperparameter _perc_ such that each real feature is compared to a specific percentile of the shadow feature set. In future work, we will allow hyperparameter optimization for _perc_. This is potentially useful when users want to compare how different feature subsets yield different modeling results (even at the cost of Type-I error when a larger feature subset is considered). 

Additionally, note that Boruta is a _greedy_ method in the sense that once a feature is deemed relevant/irrelevant in a given iteration, later iterations will not revoke this judgment. Though this helps reduce the computational complexity (by decreasing the number of trees in the random forest in later iterations), this greedy approach leaves a large room for error in hypothesis testing (particularly when $p >> n$). That is, an irrelevant feature can be _luckily_ deemed relevant in some small number of initial iterations. The converse is also true for relevant features. A less greedy approach might help lower the probabilities of type-I/type-II errors by relying less on hypothesis testing from initial iterations. This will also be examined in the future.  

# How to use the pipeline
Users are suggested to use the original codes directly if they are interested in only the result of LASSOCV or Boruta but not both. Also, the script doubleImpact.py assumes that the user has already installed the boruta_py Python package with the latest release on their [GitHub repository](https://github.com/scikit-learn-contrib/boruta_py). In particular, their boruta_py.py file can be different from the version installed by conda/pip where the latter involves deprecated Numpy commands.

Simply download/copy the script to your work directory and load the training feature set and target as X and Y respectively. After specifying a desired _scipy_ random forest estimator (i.e., classifier/regressor) as the estimator_B argument, the pipeline will return a filtered dataset with only the selected features and a summary of which feature was picked by which feature selection method. All other runtime parameters have their default values specified as those in their original implementation in sklearn or boruta_py. The bottom of the script provided a simple example. 
